{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn -q\n",
    "!pip install torchsummaryX wandb --quiet\n",
    "!pip install tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9af0051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummaryX import summary\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "af2f8bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS backend is available!\")\n",
    "else:\n",
    "    print(\"MPS backend is not available.\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "219a5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    df = pd.read_csv(\"merged_output2.csv\")\n",
    "    na_rows = df[df.isna().any(axis=1)]\n",
    "    df = df.dropna()\n",
    "    train, temp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    return train, val, test\n",
    "train, val, test = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e55d5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeros injected\n",
    "def preprocess():\n",
    "    df = pd.read_csv(\"merged_output3.csv\")\n",
    "    na_rows = df[df.isna().any(axis=1)]\n",
    "    df = df.dropna()\n",
    "    train, temp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    return train, val, test\n",
    "train, val, test = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7f3549",
   "metadata": {},
   "source": [
    "# NN MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2e835713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.x.iloc[index].values, dtype=torch.float32),\n",
    "                torch.tensor(self.y.iloc[index], dtype=torch.float32))\n",
    "\n",
    "\n",
    "training = OccupancyDataset(train.iloc[:, 2:5],  train.iloc[:, -1])\n",
    "validation = OccupancyDataset(val.iloc[:, 2:5],  val.iloc[:, -1])\n",
    "test = OccupancyDataset(test.iloc[:, 2:5],  test.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bd23d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'activations': 'GELU',\n",
    "    'learning_rate': 0.001,\n",
    "    'max_lr' : 0.006,\n",
    "    'pct_start': 0.1,\n",
    "    'optimizers': 'AdamW',\n",
    "    'scheduler': 'OneCycleLR', #'ReduceLROnPlateau'\n",
    "    'epochs': 25,\n",
    "    'batch_size': 32,\n",
    "    'weight_initialization': 'kaiming_normal', # e.g kaiming_normal, kaiming_uniform, uniform, xavier_normal or xavier_uniform\n",
    "    'dropout': 0.2\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5b3cb2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = training,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = True,\n",
    ")\n",
    "\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = validation,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = test,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9bdabc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "all = []\n",
    "for i, data in enumerate(val_loader):\n",
    "    sensor_data, target = data\n",
    "    all.append(target)\n",
    "    print(len(sensor_data))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e5c9b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.ModuleList()\n",
    "\n",
    "        #input layer\n",
    "        self.sequential.append(nn.Linear(input_dim, hidden_dim))\n",
    "        self.sequential.append(nn.ReLU())\n",
    "        self.sequential.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        #hidden\n",
    "        for i in range(layers):\n",
    "            self.sequential.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.sequential.append(nn.ReLU())\n",
    "            self.sequential.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        #output layer\n",
    "        self.sequential.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.sequential.append(nn.ReLU())\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                if config[\"weight_initialization\"] == \"xavier_normal\":\n",
    "                    torch.nn.init.xavier_normal_(m.weight)\n",
    "                elif config[\"weight_initialization\"] == \"xavier_uniform\":\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                elif config[\"weight_initialization\"] == \"kaiming_normal\":\n",
    "                    torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                elif config[\"weight_initialization\"] == \"kaiming_uniform\":\n",
    "                    torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                elif config[\"weight_initialization\"] == \"uniform\":\n",
    "                    torch.nn.init.uniform_(m.weight)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid weight_initialization value\")\n",
    "                m.bias.data.fill_(0)\n",
    "    def forward(self, x):\n",
    "        for layer in self.sequential:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "222d1449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Layer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n",
      "====================================================================================================\n",
      "0_Linear                      [3, 8]              [32, 8]                 0.03                 0.00\n",
      "1_ReLU                             -              [32, 8]                    -                    -\n",
      "2_BatchNorm1d                    [8]              [32, 8]                 0.02                 0.00\n",
      "3_Linear                      [8, 8]              [32, 8]                 0.07                 0.00\n",
      "4_ReLU                             -              [32, 8]                    -                    -\n",
      "5_BatchNorm1d                    [8]              [32, 8]                 0.02                 0.00\n",
      "6_Linear                      [8, 8]              [32, 8]                 0.07                 0.00\n",
      "7_ReLU                             -              [32, 8]                    -                    -\n",
      "8_BatchNorm1d                    [8]              [32, 8]                 0.02                 0.00\n",
      "9_Linear                      [8, 8]              [32, 8]                 0.07                 0.00\n",
      "10_ReLU                            -              [32, 8]                    -                    -\n",
      "11_BatchNorm1d                   [8]              [32, 8]                 0.02                 0.00\n",
      "12_Linear                     [8, 1]              [32, 1]                 0.01                 0.00\n",
      "13_ReLU                            -              [32, 1]                    -                    -\n",
      "====================================================================================================\n",
      "# Params:    0.32K\n",
      "# Mult-Adds: 0.00M\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_dim=3, \n",
    "            hidden_dim=8, \n",
    "            output_dim=1, \n",
    "            layers = 3, \n",
    "            dropout_rate= config[\"dropout\"]).to(device)\n",
    "summary(model, sensor_data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c43ecb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                       max_lr = config['max_lr'], \n",
    "                                                       pct_start = config['pct_start'], \n",
    "                                                       steps_per_epoch=len(train_loader),\n",
    "                                                       anneal_strategy = 'cos',\n",
    "                                                       epochs=config[\"epochs\"]\n",
    "                                                       )\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b7fa1c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/janbol/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(\n",
    "\n",
    "    key=\"f449e5715dbf82026aae85dffabb14116b5aa142\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1a37054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    tloss = 0.0\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, desc=\"Training\")\n",
    "    for i, (input, target) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input = input.to(device)\n",
    "        target = target.to(device).unsqueeze(1)\n",
    "        #print(target.shape)\n",
    "\n",
    "        logits = model(input)\n",
    "        #print(logits.shape)\n",
    "        loss = criterion(logits, target)\n",
    "        tloss+=loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        batch_bar.update()\n",
    "        scheduler.step()\n",
    "    batch_bar.close()\n",
    "    tloss /= len(train_loader)\n",
    "    return tloss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ac2aa9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    vloss = 0.0\n",
    "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Validation')\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input = input.to(device)\n",
    "            target = target.to(device).unsqueeze(1)\n",
    "            \n",
    "\n",
    "            logits = model(input)\n",
    "            loss = criterion(logits, target)\n",
    "            vloss+=loss.item()\n",
    "\n",
    "            batch_bar.update()\n",
    "            #print(\"logits shape:\", logits.shape)\n",
    "            #print(\"target shape:\", target.shape)\n",
    "\n",
    "    batch_bar.close()\n",
    "    vloss /= len(val_loader)\n",
    "    return vloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "db4d2a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/janbol/Downloads/GitHub/hvac_rnn/wandb/run-20250505_000611-7jz2boip</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project/runs/7jz2boip' target=\"_blank\">run_zeros_injected</a></strong> to <a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project' target=\"_blank\">https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project/runs/7jz2boip' target=\"_blank\">https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project/runs/7jz2boip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"autonomous_project\",  # Specify your project\n",
    "    config = config,\n",
    "    name = \"run_zeros_injected\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ea54cc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:01<00:00, 98.52it/s] \n",
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 8.0798\t Learning Rate 0.0002400\n",
      "\tVal Loss 5.1228\n",
      "saving\n",
      "\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 144.86it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 3.2335\t Learning Rate 0.0022413\n",
      "\tVal Loss 0.4062\n",
      "saving\n",
      "\n",
      "Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 151.64it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.8214\t Learning Rate 0.0054638\n",
      "\tVal Loss 0.3525\n",
      "saving\n",
      "\n",
      "Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 146.58it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.6865\t Learning Rate 0.0059925\n",
      "\tVal Loss 0.7033\n",
      "\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 145.82it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.7112\t Learning Rate 0.0059337\n",
      "\tVal Loss 1.3240\n",
      "\n",
      "Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 145.30it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.5930\t Learning Rate 0.0058179\n",
      "\tVal Loss 0.3522\n",
      "saving\n",
      "\n",
      "Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 145.42it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.6848\t Learning Rate 0.0056472\n",
      "\tVal Loss 0.3406\n",
      "saving\n",
      "\n",
      "Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 147.46it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.5351\t Learning Rate 0.0054251\n",
      "\tVal Loss 0.3318\n",
      "saving\n",
      "\n",
      "Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 148.75it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.4324\t Learning Rate 0.0051557\n",
      "\tVal Loss 0.2622\n",
      "saving\n",
      "\n",
      "Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 144.34it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2324\t Learning Rate 0.0048443\n",
      "\tVal Loss 0.2564\n",
      "saving\n",
      "\n",
      "Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 148.97it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2346\t Learning Rate 0.0044971\n",
      "\tVal Loss 0.2538\n",
      "saving\n",
      "\n",
      "Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 149.41it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2297\t Learning Rate 0.0041207\n",
      "\tVal Loss 0.2476\n",
      "saving\n",
      "\n",
      "Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 149.74it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2293\t Learning Rate 0.0037225\n",
      "\tVal Loss 0.2471\n",
      "saving\n",
      "\n",
      "Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 150.96it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2268\t Learning Rate 0.0033102\n",
      "\tVal Loss 0.2480\n",
      "\n",
      "Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 142.82it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2258\t Learning Rate 0.0028919\n",
      "\tVal Loss 0.2431\n",
      "saving\n",
      "\n",
      "Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 145.35it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2252\t Learning Rate 0.0024757\n",
      "\tVal Loss 0.2413\n",
      "saving\n",
      "\n",
      "Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 142.83it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2257\t Learning Rate 0.0020697\n",
      "\tVal Loss 0.2402\n",
      "saving\n",
      "\n",
      "Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 145.81it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2217\t Learning Rate 0.0016818\n",
      "\tVal Loss 0.2373\n",
      "saving\n",
      "\n",
      "Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 145.92it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2193\t Learning Rate 0.0013196\n",
      "\tVal Loss 0.2373\n",
      "\n",
      "Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 146.30it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2184\t Learning Rate 0.0009901\n",
      "\tVal Loss 0.2343\n",
      "saving\n",
      "\n",
      "Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 148.41it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2203\t Learning Rate 0.0006997\n",
      "\tVal Loss 0.2334\n",
      "saving\n",
      "\n",
      "Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 145.77it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2213\t Learning Rate 0.0004541\n",
      "\tVal Loss 0.2361\n",
      "\n",
      "Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 144.93it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2202\t Learning Rate 0.0002580\n",
      "\tVal Loss 0.2336\n",
      "\n",
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 149.78it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2198\t Learning Rate 0.0001153\n",
      "\tVal Loss 0.2375\n",
      "\n",
      "Epoch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:00<00:00, 144.06it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25\n",
      "\tTrain Loss 0.2178\t Learning Rate 0.0000287\n",
      "\tVal Loss 0.2341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "loss = float(\"inf\")\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
    "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
    "    train_loss   = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss       = eval(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{config['epochs']}\", flush=True)\n",
    "    print(f\"\\tTrain Loss {train_loss:.04f}\\t Learning Rate {curr_lr:.07f}\")\n",
    "    print(f\"\\tVal Loss {val_loss:.04f}\")\n",
    "\n",
    "    wandb.log({'train_loss': train_loss,\n",
    "               'valid_loss': val_loss, 'lr': curr_lr})\n",
    "    if val_loss<loss:\n",
    "        loss = val_loss\n",
    "        print(\"saving\")\n",
    "\n",
    "        torch.save(model.state_dict(), \"best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1b7f84dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>▁▄▇████▇▇▇▆▆▅▅▄▄▃▃▃▂▂▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss</td><td>█▁▁▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>3e-05</td></tr><tr><td>train_loss</td><td>0.21779</td></tr><tr><td>valid_loss</td><td>0.23406</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_zeros_injected</strong> at: <a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project/runs/7jz2boip' target=\"_blank\">https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project/runs/7jz2boip</a><br> View project at: <a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project' target=\"_blank\">https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project</a><br>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_000611-7jz2boip/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8ff231d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            inputs = batch[0].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            test_predictions.append(np.round(logits.cpu().numpy()))\n",
    "\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40f151a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f2e46e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 237.63it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3797431d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.],\n",
       "       [4.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [4.]], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f7fd6693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Labels: [3. 4. 3. 4. 4. 4. 4. 3. 3. 4. 4. 4. 3. 4. 3. 4. 3. 4. 3. 4. 3. 4. 2. 4.\n",
      " 4. 3. 4. 3. 4. 3. 3. 4.]\n",
      "Batch 1\n",
      "Labels: [3. 4. 4. 3. 4. 4. 3. 3. 3. 4. 4. 3. 4. 3. 4. 3. 3. 4. 2. 4. 4. 4. 4. 4.\n",
      " 4. 2. 4. 4. 4. 4. 4. 4.]\n",
      "Batch 2\n",
      "Labels: [3. 4. 4. 2. 3. 3. 3. 3. 3. 4. 4. 4. 4. 4. 3. 4. 3. 4. 3. 3. 3. 4. 4. 4.\n",
      " 2. 3. 3. 4. 3. 3. 3. 3.]\n",
      "Batch 3\n",
      "Labels: [4. 3. 3. 4. 3. 4. 4. 2. 4. 4. 3. 4. 3. 2. 3. 4. 3. 4. 4. 4. 4. 3. 4. 4.\n",
      " 4. 3. 4. 3. 3. 3. 2. 4.]\n",
      "Batch 4\n",
      "Labels: [4. 4. 4. 4. 2. 3. 4. 4. 4. 3. 4. 3. 4. 3. 4. 3. 4. 3. 3. 3. 4. 3. 4. 4.\n",
      " 3. 4. 4. 4. 3. 4. 4. 4.]\n",
      "Batch 5\n",
      "Labels: [3. 4. 3. 3. 4. 3. 4. 3. 4. 4. 4. 4. 4. 4. 2. 3. 3. 4. 4. 4. 4. 3. 4. 4.\n",
      " 3. 2. 3. 3. 3. 3. 4. 4.]\n",
      "Batch 6\n",
      "Labels: [4. 4. 4. 4. 3. 3. 3. 4. 4. 3. 3. 3. 4. 3. 4. 4. 4. 4. 4. 3. 3. 4. 3. 3.\n",
      " 3. 4. 4. 3. 3. 4. 4. 3.]\n",
      "Batch 7\n",
      "Labels: [3. 3. 2. 4. 4. 3. 3. 3. 4. 3. 3. 3. 4. 3. 2. 4. 4. 4. 4. 2. 3. 4. 4. 3.\n",
      " 3. 3. 3. 4. 4. 4. 3. 3.]\n",
      "Batch 8\n",
      "Labels: [3. 4. 4. 4. 2. 4. 3. 4. 3. 4. 3. 4. 4. 4. 4. 4. 4. 3. 3. 3. 2. 4. 3. 3.\n",
      " 4. 3. 3. 4. 4. 4. 4. 3.]\n",
      "Batch 9\n",
      "Labels: [3. 3. 3. 3. 4. 3. 4. 2. 4. 3. 3. 4. 4. 2. 4. 4. 3. 4. 4. 3. 4. 3. 3. 3.\n",
      " 4. 2. 4. 3. 3. 3. 4. 3.]\n",
      "Batch 10\n",
      "Labels: [3. 4. 4. 4. 3. 4. 3. 4. 4. 3. 4. 4. 2. 3. 3. 4. 3. 3. 4. 2. 4. 4. 2. 3.\n",
      " 4. 4. 3. 3. 3. 4. 4. 4.]\n",
      "Batch 11\n",
      "Labels: [4. 3. 4. 3. 3. 4. 4. 4. 3. 3. 3. 3. 3. 3. 4. 3. 4. 4. 3. 2. 4. 4. 3. 3.\n",
      " 3. 4. 3. 4. 4. 3. 3. 4.]\n",
      "Batch 12\n",
      "Labels: [4. 2. 4. 4. 5. 3. 3. 3. 4. 3. 4. 3. 3. 3. 4. 3. 3. 4. 4. 3. 3. 3. 3. 3.\n",
      " 3. 2. 4. 4. 4. 3. 3. 3.]\n",
      "Batch 13\n",
      "Labels: [4. 3. 2. 3. 3. 4. 4. 3. 4. 4. 3. 2. 3. 3. 4. 3. 4. 3. 4. 2. 4. 3. 4. 4.\n",
      " 3. 4. 2. 4. 4. 4. 4. 4.]\n",
      "Batch 14\n",
      "Labels: [3. 3. 3. 4. 3. 3. 4. 4. 3. 3. 4. 3. 4. 3. 3. 3. 4. 4. 3. 5. 3. 4. 3. 4.\n",
      " 4. 3. 5. 3. 3. 3. 3. 3.]\n",
      "Batch 15\n",
      "Labels: [4. 4. 3. 3. 4. 3. 3. 3. 4. 2. 3. 4. 2. 4. 3. 3. 3. 3. 3. 2. 4. 3. 3. 4.\n",
      " 3. 3. 3. 4. 4. 4. 2. 3.]\n",
      "Batch 16\n",
      "Labels: [4. 3. 2. 4. 4. 4. 4. 3. 4. 4. 3. 3. 4. 3. 3. 4. 4. 4. 2. 4. 4. 3. 4. 4.\n",
      " 3. 4. 3. 4. 4. 4. 3. 4.]\n",
      "Batch 17\n",
      "Labels: [2. 2. 4. 4. 4. 3. 4. 4. 4. 2. 3. 3. 4. 4. 4. 4. 2. 3. 4. 3. 4. 4. 3. 3.\n",
      " 2. 5. 4. 4. 3. 4. 4. 4.]\n",
      "Batch 18\n",
      "Labels: [3. 3. 4. 4. 4. 4. 3. 4. 3. 4. 3. 3. 4. 2. 4. 3. 3. 3. 4. 4. 3. 3. 2. 3.\n",
      " 4. 5. 2. 3. 3. 3. 3. 3.]\n",
      "Batch 19\n",
      "Labels: [4. 3. 3. 3. 3. 4. 4. 3. 3. 3. 3. 3. 4. 3. 4. 3. 4. 3. 4. 4. 4. 4. 3. 4.\n",
      " 3. 3. 4. 3. 3. 4. 3. 4.]\n",
      "Batch 20\n",
      "Labels: [4. 4. 3. 4. 3. 4. 4. 3. 4. 3. 2. 3. 4. 4. 4. 3. 4. 4. 4. 3. 3. 3. 4. 3.\n",
      " 4. 2. 3. 4. 4. 3. 4. 3.]\n",
      "Batch 21\n",
      "Labels: [2. 3. 4. 4. 4. 4. 4. 4. 3. 3. 3. 3. 3. 4. 3. 4. 3. 3. 4. 3. 4. 3. 3. 3.\n",
      " 3. 4. 4. 3. 4. 4. 4. 4.]\n",
      "Batch 22\n",
      "Labels: [4. 3. 4. 3. 3. 4. 3. 4. 3. 3. 2. 4. 4. 3. 3. 3. 2. 2. 4. 4. 4. 4. 4. 4.\n",
      " 3. 4. 3. 4. 3. 3. 3. 2.]\n",
      "Batch 23\n",
      "Labels: [4. 3. 3. 3. 4. 4. 3. 4. 3. 4. 4. 3. 4. 4. 4. 3. 4. 4. 4. 3. 4. 4. 3. 4.\n",
      " 4. 4. 4. 4. 2. 4. 3. 3.]\n",
      "Batch 24\n",
      "Labels: [4. 4. 3. 4. 4. 2. 3. 3. 3. 3. 4. 4. 2. 3. 4. 3. 4. 4. 4. 3. 4. 4. 4. 2.\n",
      " 3. 4. 3. 4. 4. 3. 4. 2.]\n",
      "Batch 25\n",
      "Labels: [3. 3. 4. 3. 4. 4. 3. 3. 3. 3. 3. 4. 3. 4. 2. 3. 3. 3. 4. 4. 4. 3. 3. 4.\n",
      " 3. 3. 4. 3. 4. 4. 4. 4.]\n",
      "Batch 26\n",
      "Labels: [4. 3. 4. 3.]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "    print(f\"Batch {batch_idx}\")\n",
    "    print(\"Labels:\", labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99451416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "bfccda74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.188081]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the model correctly\n",
    "state_dict = torch.load('best_model')  # Load the saved state dict\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Assuming 'model' is your trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Correctly pass the input as a tensor\n",
    "    logits = model(torch.tensor([3.0, 6.0, 4.0]).unsqueeze(0).to(device))  # Make sure to cast to float tensor\n",
    "    rounded_logits = (logits.cpu().numpy())  # Round the logits\n",
    "\n",
    "print(rounded_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617e61ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
