{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn -q\n",
    "!pip install torchsummaryX wandb --quiet\n",
    "!pip install tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9af0051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummaryX import summary\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af2f8bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS backend is available!\")\n",
    "else:\n",
    "    print(\"MPS backend is not available.\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "219a5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    df = pd.read_csv(\"merged_output2.csv\")\n",
    "    na_rows = df[df.isna().any(axis=1)]\n",
    "    df = df.dropna()\n",
    "    train, temp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    return train, val, test\n",
    "train, val, test = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7f3549",
   "metadata": {},
   "source": [
    "# NN MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e835713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.x.iloc[index].values, dtype=torch.float32),\n",
    "                torch.tensor(self.y.iloc[index], dtype=torch.float32))\n",
    "\n",
    "\n",
    "training = OccupancyDataset(train.iloc[:, 2:5],  train.iloc[:, -1])\n",
    "validation = OccupancyDataset(val.iloc[:, 2:5],  val.iloc[:, -1])\n",
    "test = OccupancyDataset(test.iloc[:, 2:5],  test.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bd23d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'activations': 'GELU',\n",
    "    'learning_rate': 0.001,\n",
    "    'max_lr' : 0.006,\n",
    "    'pct_start': 0.1,\n",
    "    'optimizers': 'AdamW',\n",
    "    'scheduler': 'OneCycleLR', #'ReduceLROnPlateau'\n",
    "    'epochs': 25,\n",
    "    'batch_size': 32,\n",
    "    'weight_initialization': 'kaiming_normal', # e.g kaiming_normal, kaiming_uniform, uniform, xavier_normal or xavier_uniform\n",
    "    'dropout': 0.2\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b3cb2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = training,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = True,\n",
    ")\n",
    "\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = validation,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = test,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9bdabc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "all = []\n",
    "for i, data in enumerate(val_loader):\n",
    "    sensor_data, target = data\n",
    "    all.append(target)\n",
    "    print(len(sensor_data))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e5c9b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.ModuleList()\n",
    "\n",
    "        #input layer\n",
    "        self.sequential.append(nn.Linear(input_dim, hidden_dim))\n",
    "        self.sequential.append(nn.ReLU())\n",
    "        self.sequential.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        #hidden\n",
    "        for i in range(layers):\n",
    "            self.sequential.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.sequential.append(nn.ReLU())\n",
    "            self.sequential.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        #output layer\n",
    "        self.sequential.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                if config[\"weight_initialization\"] == \"xavier_normal\":\n",
    "                    torch.nn.init.xavier_normal_(m.weight)\n",
    "                elif config[\"weight_initialization\"] == \"xavier_uniform\":\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                elif config[\"weight_initialization\"] == \"kaiming_normal\":\n",
    "                    torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                elif config[\"weight_initialization\"] == \"kaiming_uniform\":\n",
    "                    torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                elif config[\"weight_initialization\"] == \"uniform\":\n",
    "                    torch.nn.init.uniform_(m.weight)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid weight_initialization value\")\n",
    "                m.bias.data.fill_(0)\n",
    "    def forward(self, x):\n",
    "        for layer in self.sequential:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "222d1449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Layer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n",
      "====================================================================================================\n",
      "0_Linear                     [3, 30]             [32, 30]                 0.12                 0.00\n",
      "1_ReLU                             -             [32, 30]                    -                    -\n",
      "2_BatchNorm1d                   [30]             [32, 30]                 0.06                 0.00\n",
      "3_Linear                    [30, 30]             [32, 30]                 0.93                 0.00\n",
      "4_ReLU                             -             [32, 30]                    -                    -\n",
      "5_BatchNorm1d                   [30]             [32, 30]                 0.06                 0.00\n",
      "6_Linear                    [30, 30]             [32, 30]                 0.93                 0.00\n",
      "7_ReLU                             -             [32, 30]                    -                    -\n",
      "8_BatchNorm1d                   [30]             [32, 30]                 0.06                 0.00\n",
      "9_Linear                    [30, 30]             [32, 30]                 0.93                 0.00\n",
      "10_ReLU                            -             [32, 30]                    -                    -\n",
      "11_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "12_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "13_ReLU                            -             [32, 30]                    -                    -\n",
      "14_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "15_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "16_ReLU                            -             [32, 30]                    -                    -\n",
      "17_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "18_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "19_ReLU                            -             [32, 30]                    -                    -\n",
      "20_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "21_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "22_ReLU                            -             [32, 30]                    -                    -\n",
      "23_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "24_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "25_ReLU                            -             [32, 30]                    -                    -\n",
      "26_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "27_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "28_ReLU                            -             [32, 30]                    -                    -\n",
      "29_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "30_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "31_ReLU                            -             [32, 30]                    -                    -\n",
      "32_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "33_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "34_ReLU                            -             [32, 30]                    -                    -\n",
      "35_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "36_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "37_ReLU                            -             [32, 30]                    -                    -\n",
      "38_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "39_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "40_ReLU                            -             [32, 30]                    -                    -\n",
      "41_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "42_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "43_ReLU                            -             [32, 30]                    -                    -\n",
      "44_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "45_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "46_ReLU                            -             [32, 30]                    -                    -\n",
      "47_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "48_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "49_ReLU                            -             [32, 30]                    -                    -\n",
      "50_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "51_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "52_ReLU                            -             [32, 30]                    -                    -\n",
      "53_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "54_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "55_ReLU                            -             [32, 30]                    -                    -\n",
      "56_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "57_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "58_ReLU                            -             [32, 30]                    -                    -\n",
      "59_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "60_Linear                   [30, 30]             [32, 30]                 0.93                 0.00\n",
      "61_ReLU                            -             [32, 30]                    -                    -\n",
      "62_BatchNorm1d                  [30]             [32, 30]                 0.06                 0.00\n",
      "63_Linear                    [30, 1]              [32, 1]                 0.03                 0.00\n",
      "====================================================================================================\n",
      "# Params:    20.01K\n",
      "# Mult-Adds: 0.02M\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_dim=3, \n",
    "            hidden_dim=30, \n",
    "            output_dim=1, \n",
    "            layers = 20, \n",
    "            dropout_rate= config[\"dropout\"]).to(device)\n",
    "summary(model, sensor_data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c43ecb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                       max_lr = config['max_lr'], \n",
    "                                                       pct_start = config['pct_start'], \n",
    "                                                       steps_per_epoch=len(train_loader),\n",
    "                                                       anneal_strategy = 'cos',\n",
    "                                                       epochs=config[\"epochs\"]\n",
    "                                                       )\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b7fa1c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(\n",
    "\n",
    "    key=\"f449e5715dbf82026aae85dffabb14116b5aa142\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1a37054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    tloss = 0.0\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, desc=\"Training\")\n",
    "    for i, (input, target) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input = input.to(device)\n",
    "        target = target.to(device).unsqueeze(1)\n",
    "        #print(target.shape)\n",
    "\n",
    "        logits = model(input)\n",
    "        #print(logits.shape)\n",
    "        loss = criterion(logits, target)\n",
    "        tloss+=loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        batch_bar.update()\n",
    "        scheduler.step()\n",
    "    batch_bar.close()\n",
    "    tloss /= len(train_loader)\n",
    "    return tloss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ac2aa9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    vloss = 0.0\n",
    "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Validation')\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input = input.to(device)\n",
    "            target = target.to(device).unsqueeze(1)\n",
    "            \n",
    "\n",
    "            logits = model(input)\n",
    "            loss = criterion(logits, target)\n",
    "            vloss+=loss.item()\n",
    "\n",
    "            batch_bar.update()\n",
    "            #print(\"logits shape:\", logits.shape)\n",
    "            #print(\"target shape:\", target.shape)\n",
    "\n",
    "    batch_bar.close()\n",
    "    vloss /= len(val_loader)\n",
    "    return vloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "db4d2a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run8</strong> at: <a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project/runs/vzyus8ds' target=\"_blank\">https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project/runs/vzyus8ds</a><br> View project at: <a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project' target=\"_blank\">https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project</a><br>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250504_230027-vzyus8ds/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/janbol/Downloads/GitHub/hvac_rnn/wandb/run-20250504_230113-pik8c8mt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project/runs/pik8c8mt' target=\"_blank\">run8</a></strong> to <a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project' target=\"_blank\">https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project/runs/pik8c8mt' target=\"_blank\">https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project/runs/pik8c8mt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"autonomous_project\",  # Specify your project\n",
    "    config = config,\n",
    "    name = \"run8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ea54cc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 42.64it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 10.5270\t Learning Rate 0.0002400\n",
      "\tVal Loss 7.1907\n",
      "saving\n",
      "\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.43it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 1.8648\t Learning Rate 0.0022414\n",
      "\tVal Loss 0.3939\n",
      "saving\n",
      "\n",
      "Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 42.80it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.3057\t Learning Rate 0.0054639\n",
      "\tVal Loss 0.2701\n",
      "saving\n",
      "\n",
      "Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 42.25it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2894\t Learning Rate 0.0059925\n",
      "\tVal Loss 0.2316\n",
      "saving\n",
      "\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.93it/s]\n",
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2459\t Learning Rate 0.0059337\n",
      "\tVal Loss 0.2341\n",
      "\n",
      "Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 42.08it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2251\t Learning Rate 0.0058179\n",
      "\tVal Loss 0.1917\n",
      "saving\n",
      "\n",
      "Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.37it/s]\n",
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2300\t Learning Rate 0.0056472\n",
      "\tVal Loss 0.2045\n",
      "\n",
      "Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.00it/s]\n",
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2266\t Learning Rate 0.0054250\n",
      "\tVal Loss 0.1802\n",
      "saving\n",
      "\n",
      "Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:03<00:00, 40.21it/s]\n",
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2217\t Learning Rate 0.0051556\n",
      "\tVal Loss 0.1764\n",
      "saving\n",
      "\n",
      "Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:03<00:00, 40.42it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2136\t Learning Rate 0.0048443\n",
      "\tVal Loss 0.2255\n",
      "\n",
      "Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 42.64it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.2123\t Learning Rate 0.0044970\n",
      "\tVal Loss 0.1756\n",
      "saving\n",
      "\n",
      "Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.63it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1954\t Learning Rate 0.0041206\n",
      "\tVal Loss 0.1605\n",
      "saving\n",
      "\n",
      "Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.96it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1810\t Learning Rate 0.0037224\n",
      "\tVal Loss 0.1623\n",
      "\n",
      "Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 42.93it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1796\t Learning Rate 0.0033102\n",
      "\tVal Loss 0.1699\n",
      "\n",
      "Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 42.54it/s]\n",
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1780\t Learning Rate 0.0028919\n",
      "\tVal Loss 0.1609\n",
      "\n",
      "Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.39it/s]\n",
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1778\t Learning Rate 0.0024757\n",
      "\tVal Loss 0.1587\n",
      "saving\n",
      "\n",
      "Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.40it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1764\t Learning Rate 0.0020697\n",
      "\tVal Loss 0.1607\n",
      "\n",
      "Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.61it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1741\t Learning Rate 0.0016818\n",
      "\tVal Loss 0.1562\n",
      "saving\n",
      "\n",
      "Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.54it/s]\n",
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1677\t Learning Rate 0.0013196\n",
      "\tVal Loss 0.1559\n",
      "saving\n",
      "\n",
      "Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.00it/s]\n",
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1743\t Learning Rate 0.0009901\n",
      "\tVal Loss 0.1579\n",
      "\n",
      "Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.83it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1590\t Learning Rate 0.0006997\n",
      "\tVal Loss 0.1514\n",
      "saving\n",
      "\n",
      "Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 42.04it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1623\t Learning Rate 0.0004541\n",
      "\tVal Loss 0.1555\n",
      "\n",
      "Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.24it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1608\t Learning Rate 0.0002580\n",
      "\tVal Loss 0.1492\n",
      "saving\n",
      "\n",
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 42.26it/s]\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 0.1554\t Learning Rate 0.0001153\n",
      "\tVal Loss 0.1515\n",
      "\n",
      "Epoch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [00:02<00:00, 41.12it/s]\n",
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25\n",
      "\tTrain Loss 0.1641\t Learning Rate 0.0000287\n",
      "\tVal Loss 0.1515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "loss = float(\"inf\")\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
    "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
    "    train_loss   = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss       = eval(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{config['epochs']}\", flush=True)\n",
    "    print(f\"\\tTrain Loss {train_loss:.04f}\\t Learning Rate {curr_lr:.07f}\")\n",
    "    print(f\"\\tVal Loss {val_loss:.04f}\")\n",
    "\n",
    "    wandb.log({'train_loss': train_loss,\n",
    "               'valid_loss': val_loss, 'lr': curr_lr})\n",
    "    if val_loss<loss:\n",
    "        loss = val_loss\n",
    "        print(\"saving\")\n",
    "\n",
    "        torch.save(model.state_dict(), \"best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1b7f84dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>▁▄▇████▇▇▇▆▆▅▅▄▄▃▃▃▂▂▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>3e-05</td></tr><tr><td>train_loss</td><td>0.16407</td></tr><tr><td>valid_loss</td><td>0.15147</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run8</strong> at: <a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project/runs/pik8c8mt' target=\"_blank\">https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project/runs/pik8c8mt</a><br> View project at: <a href='https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project' target=\"_blank\">https://wandb.ai/janbol-carnegie-mellon-university/autonomous_project</a><br>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250504_230113-pik8c8mt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8ff231d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            inputs = batch[0].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            test_predictions.append(np.round(logits.cpu().numpy()))\n",
    "\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40f151a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f2e46e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 237.63it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3797431d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.],\n",
       "       [4.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [4.]], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f7fd6693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Labels: [3. 4. 3. 4. 4. 4. 4. 3. 3. 4. 4. 4. 3. 4. 3. 4. 3. 4. 3. 4. 3. 4. 2. 4.\n",
      " 4. 3. 4. 3. 4. 3. 3. 4.]\n",
      "Batch 1\n",
      "Labels: [3. 4. 4. 3. 4. 4. 3. 3. 3. 4. 4. 3. 4. 3. 4. 3. 3. 4. 2. 4. 4. 4. 4. 4.\n",
      " 4. 2. 4. 4. 4. 4. 4. 4.]\n",
      "Batch 2\n",
      "Labels: [3. 4. 4. 2. 3. 3. 3. 3. 3. 4. 4. 4. 4. 4. 3. 4. 3. 4. 3. 3. 3. 4. 4. 4.\n",
      " 2. 3. 3. 4. 3. 3. 3. 3.]\n",
      "Batch 3\n",
      "Labels: [4. 3. 3. 4. 3. 4. 4. 2. 4. 4. 3. 4. 3. 2. 3. 4. 3. 4. 4. 4. 4. 3. 4. 4.\n",
      " 4. 3. 4. 3. 3. 3. 2. 4.]\n",
      "Batch 4\n",
      "Labels: [4. 4. 4. 4. 2. 3. 4. 4. 4. 3. 4. 3. 4. 3. 4. 3. 4. 3. 3. 3. 4. 3. 4. 4.\n",
      " 3. 4. 4. 4. 3. 4. 4. 4.]\n",
      "Batch 5\n",
      "Labels: [3. 4. 3. 3. 4. 3. 4. 3. 4. 4. 4. 4. 4. 4. 2. 3. 3. 4. 4. 4. 4. 3. 4. 4.\n",
      " 3. 2. 3. 3. 3. 3. 4. 4.]\n",
      "Batch 6\n",
      "Labels: [4. 4. 4. 4. 3. 3. 3. 4. 4. 3. 3. 3. 4. 3. 4. 4. 4. 4. 4. 3. 3. 4. 3. 3.\n",
      " 3. 4. 4. 3. 3. 4. 4. 3.]\n",
      "Batch 7\n",
      "Labels: [3. 3. 2. 4. 4. 3. 3. 3. 4. 3. 3. 3. 4. 3. 2. 4. 4. 4. 4. 2. 3. 4. 4. 3.\n",
      " 3. 3. 3. 4. 4. 4. 3. 3.]\n",
      "Batch 8\n",
      "Labels: [3. 4. 4. 4. 2. 4. 3. 4. 3. 4. 3. 4. 4. 4. 4. 4. 4. 3. 3. 3. 2. 4. 3. 3.\n",
      " 4. 3. 3. 4. 4. 4. 4. 3.]\n",
      "Batch 9\n",
      "Labels: [3. 3. 3. 3. 4. 3. 4. 2. 4. 3. 3. 4. 4. 2. 4. 4. 3. 4. 4. 3. 4. 3. 3. 3.\n",
      " 4. 2. 4. 3. 3. 3. 4. 3.]\n",
      "Batch 10\n",
      "Labels: [3. 4. 4. 4. 3. 4. 3. 4. 4. 3. 4. 4. 2. 3. 3. 4. 3. 3. 4. 2. 4. 4. 2. 3.\n",
      " 4. 4. 3. 3. 3. 4. 4. 4.]\n",
      "Batch 11\n",
      "Labels: [4. 3. 4. 3. 3. 4. 4. 4. 3. 3. 3. 3. 3. 3. 4. 3. 4. 4. 3. 2. 4. 4. 3. 3.\n",
      " 3. 4. 3. 4. 4. 3. 3. 4.]\n",
      "Batch 12\n",
      "Labels: [4. 2. 4. 4. 5. 3. 3. 3. 4. 3. 4. 3. 3. 3. 4. 3. 3. 4. 4. 3. 3. 3. 3. 3.\n",
      " 3. 2. 4. 4. 4. 3. 3. 3.]\n",
      "Batch 13\n",
      "Labels: [4. 3. 2. 3. 3. 4. 4. 3. 4. 4. 3. 2. 3. 3. 4. 3. 4. 3. 4. 2. 4. 3. 4. 4.\n",
      " 3. 4. 2. 4. 4. 4. 4. 4.]\n",
      "Batch 14\n",
      "Labels: [3. 3. 3. 4. 3. 3. 4. 4. 3. 3. 4. 3. 4. 3. 3. 3. 4. 4. 3. 5. 3. 4. 3. 4.\n",
      " 4. 3. 5. 3. 3. 3. 3. 3.]\n",
      "Batch 15\n",
      "Labels: [4. 4. 3. 3. 4. 3. 3. 3. 4. 2. 3. 4. 2. 4. 3. 3. 3. 3. 3. 2. 4. 3. 3. 4.\n",
      " 3. 3. 3. 4. 4. 4. 2. 3.]\n",
      "Batch 16\n",
      "Labels: [4. 3. 2. 4. 4. 4. 4. 3. 4. 4. 3. 3. 4. 3. 3. 4. 4. 4. 2. 4. 4. 3. 4. 4.\n",
      " 3. 4. 3. 4. 4. 4. 3. 4.]\n",
      "Batch 17\n",
      "Labels: [2. 2. 4. 4. 4. 3. 4. 4. 4. 2. 3. 3. 4. 4. 4. 4. 2. 3. 4. 3. 4. 4. 3. 3.\n",
      " 2. 5. 4. 4. 3. 4. 4. 4.]\n",
      "Batch 18\n",
      "Labels: [3. 3. 4. 4. 4. 4. 3. 4. 3. 4. 3. 3. 4. 2. 4. 3. 3. 3. 4. 4. 3. 3. 2. 3.\n",
      " 4. 5. 2. 3. 3. 3. 3. 3.]\n",
      "Batch 19\n",
      "Labels: [4. 3. 3. 3. 3. 4. 4. 3. 3. 3. 3. 3. 4. 3. 4. 3. 4. 3. 4. 4. 4. 4. 3. 4.\n",
      " 3. 3. 4. 3. 3. 4. 3. 4.]\n",
      "Batch 20\n",
      "Labels: [4. 4. 3. 4. 3. 4. 4. 3. 4. 3. 2. 3. 4. 4. 4. 3. 4. 4. 4. 3. 3. 3. 4. 3.\n",
      " 4. 2. 3. 4. 4. 3. 4. 3.]\n",
      "Batch 21\n",
      "Labels: [2. 3. 4. 4. 4. 4. 4. 4. 3. 3. 3. 3. 3. 4. 3. 4. 3. 3. 4. 3. 4. 3. 3. 3.\n",
      " 3. 4. 4. 3. 4. 4. 4. 4.]\n",
      "Batch 22\n",
      "Labels: [4. 3. 4. 3. 3. 4. 3. 4. 3. 3. 2. 4. 4. 3. 3. 3. 2. 2. 4. 4. 4. 4. 4. 4.\n",
      " 3. 4. 3. 4. 3. 3. 3. 2.]\n",
      "Batch 23\n",
      "Labels: [4. 3. 3. 3. 4. 4. 3. 4. 3. 4. 4. 3. 4. 4. 4. 3. 4. 4. 4. 3. 4. 4. 3. 4.\n",
      " 4. 4. 4. 4. 2. 4. 3. 3.]\n",
      "Batch 24\n",
      "Labels: [4. 4. 3. 4. 4. 2. 3. 3. 3. 3. 4. 4. 2. 3. 4. 3. 4. 4. 4. 3. 4. 4. 4. 2.\n",
      " 3. 4. 3. 4. 4. 3. 4. 2.]\n",
      "Batch 25\n",
      "Labels: [3. 3. 4. 3. 4. 4. 3. 3. 3. 3. 3. 4. 3. 4. 2. 3. 3. 3. 4. 4. 4. 3. 3. 4.\n",
      " 3. 3. 4. 3. 4. 4. 4. 4.]\n",
      "Batch 26\n",
      "Labels: [4. 3. 4. 3.]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "    print(f\"Batch {batch_idx}\")\n",
    "    print(\"Labels:\", labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99451416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bfccda74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the model correctly\n",
    "state_dict = torch.load('best_model')  # Load the saved state dict\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Assuming 'model' is your trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Correctly pass the input as a tensor\n",
    "    logits = model(torch.tensor([3.0, 4.0, 3.0]).unsqueeze(0).to(device))  # Make sure to cast to float tensor\n",
    "    rounded_logits = np.round(logits.cpu().numpy())  # Round the logits\n",
    "\n",
    "print(rounded_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617e61ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
